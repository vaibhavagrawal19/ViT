{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as opt\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor\nimport math\n\nfrom tqdm import tqdm, trange\n\n\nfrom torchvision.transforms import ToTensor\nfrom torchvision.datasets.mnist import MNIST\n\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_dim: tuple, patch_dim: tuple, in_channels: int, embed_dim: int):\n        super().__init__()\n        self.embed_dim = embed_dim\n        padding_x = (patch_dim[0] - (image_dim[0] % patch_dim[0])) % patch_dim[0]\n        padding_y = (patch_dim[1] - (image_dim[1] % patch_dim[1])) % patch_dim[1]\n        self.padding = (padding_x, padding_y)\n        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_dim, stride=patch_dim, padding=self.padding)\n\n    def forward(self, x):\n        n = x.shape[0]\n        x = self.projection(x)\n        x = x.flatten(start_dim=-2)\n        x = torch.transpose(x, -2, -1)\n        return x\n\n\n\nclass ViT(nn.Module):\n    def __init__(self, image_dim: tuple, device, in_channels=3, n_encoders=1, patch_dim=(16, 16), hidden_dim=512, n_heads=8, out_dim=10):\n        super().__init__()\n        self.image_dim = image_dim\n        self.in_channels = in_channels\n        self.patch_dim = patch_dim\n        self.n_heads = n_heads\n        self.out_dim = out_dim\n        self.device = device\n        self.n_encoders = n_encoders\n        n_patches_x = image_dim[0] // patch_dim[0] if image_dim[0] % patch_dim[0] == 0 else image_dim[0] // patch_dim[0] + 1\n        n_patches_y = image_dim[1] // patch_dim[1] if image_dim[1] % patch_dim[1] == 0 else image_dim[1] // patch_dim[1] + 1\n        self.n_patches = n_patches_x * n_patches_y\n        self.hidden_dim = hidden_dim\n\n        \"\"\"\n        # USE THIS WHEN YOU WANT THE SINE-COSINE POSITION EMBEDDINGS INSTEAD OF THE LEARNED POSITION EMBEDDINGS\n        \n        self.pos_embed = nn.Parameter(self.get_pos_embed(self.n_patches + 1))\n        self.pos_embed.requires_grad = False\n\n        \"\"\"\n        self.pos_embed = nn.Parameter(torch.rand(1, self.n_patches + 1, self.hidden_dim))\n        self.class_token = nn.Parameter(torch.rand((1, self.hidden_dim)))\n        self.encoders = nn.ModuleList([Encoder(self.hidden_dim, self.n_heads) for _ in range(self.n_encoders)])\n        self.encoders = nn.Sequential(*(self.encoders))\n        self.mlp = nn.Linear(self.hidden_dim, self.out_dim)\n        self.patchify = PatchEmbedding(self.image_dim, self.patch_dim, self.in_channels, self.hidden_dim)\n\n    def get_pos_embed(self, n_patches: int):\n        result = torch.empty((n_patches, self.hidden_dim))\n        for i in range(n_patches):\n            for j in range(self.hidden_dim):\n                result[i][j] = math.sin(i / (10000 ** (j / self.hidden_dim))) if j % 2 == 0 else math.cos(i / (10000 ** ((j - 1) / self.hidden_dim)))\n        return result\n    \n    def forward(self, x):\n        n = x.shape[0]\n        patch_embeddings = self.patchify(x)\n        assert patch_embeddings.shape == (n, self.n_patches, self.hidden_dim)\n        embeddings = torch.empty(n, patch_embeddings.shape[1] + 1, self.hidden_dim).to(self.device)\n        for i in range(n):\n            embeddings[i] = torch.cat([patch_embeddings[i], self.class_token])\n            embeddings[i] = embeddings[i] + self.pos_embed\n        features = self.encoders(patch_embeddings)[:, 0]\n        return self.mlp(features)\n        \n        \n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, hidden_dim=512, n_heads=8):\n        super().__init__()\n        assert hidden_dim % n_heads == 0\n        self.hidden_dim = hidden_dim\n        self.n_heads = n_heads\n        self.v_dim = self.hidden_dim // self.n_heads\n        self.Q = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.K = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.V = nn.Linear(self.hidden_dim, self.hidden_dim)\n        # self.Q = nn.ModuleList([nn.Linear(self.hidden_dim, self.v_dim) for _ in range(self.n_heads)])\n        # self.K = nn.ModuleList([nn.Linear(self.hidden_dim, self.v_dim) for _ in range(self.n_heads)])\n        # self.V = nn.ModuleList([nn.Linear(self.hidden_dim, self.v_dim) for _ in range(self.n_heads)])\n        self.softmax = nn.Softmax(dim=-1)\n        self.mlp = nn.Linear(self.hidden_dim, self.hidden_dim)\n\n    def attention(self, query, key, value):\n        scores = self.softmax((query @ key.transpose(-2, -1)) / math.sqrt(self.v_dim))\n        return scores @ value\n\n    def forward(self, x):\n        query = self.Q(x).reshape(x.shape[0], -1, self.n_heads, self.v_dim).transpose(1, 2)\n        key = self.K(x).reshape(x.shape[0], -1, self.n_heads, self.v_dim).transpose(1, 2)\n        value = self.V(x).reshape(x.shape[0], -1, self.n_heads, self.v_dim).transpose(1, 2)\n        x = self.attention(query, key, value)\n        x = x.transpose(1, 2).reshape(x.shape[0], -1, self.hidden_dim)\n        return self.mlp(x)\n\n    # def forward(self, sequences):\n    #     results = []\n    #     for sequence in sequences:\n    #         seq_result = []\n    #         for head in range(self.n_heads):\n    #             q = self.Q[head](sequence)\n    #             k = self.K[head](sequence)\n    #             v = self.V[head](sequence)\n    #             scores = self.softmax((q @ k.T) / math.sqrt(self.v_dim))\n    #             z = scores @ v\n    #             seq_result.append(z)\n    #         results.append(seq_result)\n    #     results = [torch.cat([head_result for head_result in seq_result], dim=-1) for seq_result in results]\n    #     results = torch.cat([result[None, :] for result in results], dim=0)\n    #     results = self.mlp(results)\n\n        # return results\n    \n\n\nclass Faster_MultiHeadAttention(nn.Module):\n    def __init__(self, hidden_dim=512, n_heads=8):\n        super().__init__()\n        assert hidden_dim % n_heads == 0\n        self.hidden_dim = hidden_dim\n        self.n_heads = n_heads\n        self.v_dim = self.hidden_dim // self.n_heads\n        self.Q = nn.ModuleList([nn.Linear(self.hidden_dim, self.v_dim) for _ in range(self.n_heads)])\n        self.K = nn.ModuleList([nn.Linear(self.hidden_dim, self.v_dim) for _ in range(self.n_heads)])\n        self.V = nn.ModuleList([nn.Linear(self.hidden_dim, self.v_dim) for _ in range(self.n_heads)])\n        self.softmax = nn.Softmax(dim=-1)\n        self.mlp = nn.Linear(self.hidden_dim, self.hidden_dim)\n\n    def forward(self, sequences):\n        results = []\n        for sequence in sequences:\n            seq_result = []\n            for head in range(self.n_heads):\n                q = self.Q[head](sequence)\n                k = self.K[head](sequence)\n                v = self.V[head](sequence)\n                scores = self.softmax((q @ k.T) / math.sqrt(self.v_dim))\n                z = scores @ v\n                seq_result.append(z)\n            results.append(seq_result)\n        results = [torch.cat([head_result for head_result in seq_result], dim=-1) for seq_result in results]\n        results = torch.cat([result[None, :] for result in results], dim=0)\n        results = self.mlp(results)\n        return results\n    \n\n\nclass Encoder(nn.Module):\n    def __init__(self, hidden_dim=512, n_heads=8):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.n_heads = n_heads\n        self.layer_norm = nn.LayerNorm(self.hidden_dim)\n        self.layer_norm2 = nn.LayerNorm(self.hidden_dim)\n        self.attention = MultiHeadAttention(self.hidden_dim, self.n_heads)\n        self.mlp = nn.Sequential(\n            nn.Linear(self.hidden_dim, self.hidden_dim),\n            nn.GELU(),\n            nn.Linear(self.hidden_dim, self.hidden_dim)\n        )\n\n    def forward(self, x):\n        x_ = x.clone()\n        x = self.layer_norm(x)\n        x = self.attention(x)\n        x = x + x_\n        x_ = x.clone()\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        x = x + x_\n        del x_\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:14:29.499607Z","iopub.execute_input":"2023-10-03T20:14:29.499972Z","iopub.status.idle":"2023-10-03T20:14:29.528941Z","shell.execute_reply.started":"2023-10-03T20:14:29.499943Z","shell.execute_reply":"2023-10-03T20:14:29.527716Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"LR = 5e-5\nNUM_EPOCHS = 40\nCONVERGENCE_THRESH = 5\nACC_THRESH = 1","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:14:30.080018Z","iopub.execute_input":"2023-10-03T20:14:30.080746Z","iopub.status.idle":"2023-10-03T20:14:30.085561Z","shell.execute_reply.started":"2023-10-03T20:14:30.080714Z","shell.execute_reply":"2023-10-03T20:14:30.084642Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"const_epochs = 0\nmax_acc = 0\nlast_acc = 0","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:14:30.633944Z","iopub.execute_input":"2023-10-03T20:14:30.634628Z","iopub.status.idle":"2023-10-03T20:14:30.639128Z","shell.execute_reply.started":"2023-10-03T20:14:30.634581Z","shell.execute_reply":"2023-10-03T20:14:30.638080Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    transform = ToTensor()\n    train_set = MNIST(root='./datasets', train=True, download=True, transform=transform)\n    test_set = MNIST(root='./datasets', train=False, download=True, transform=transform)\n    train_loader = DataLoader(train_set, shuffle=True, batch_size=50)\n    test_loader = DataLoader(test_set, shuffle=False, batch_size=50)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"using {device}\")\n    # device = torch.device(\"cpu\")\n    model = ViT((28, 28), device, in_channels=1, n_encoders=3, hidden_dim=512, n_heads=8, patch_dim=(7, 7)).to(device)\n    optimizer = opt.Adam(model.parameters(), lr=LR)\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        # Create a tqdm progress bar for the training batches\n        with tqdm(train_loader, unit=\"batch\") as t_bar:\n            for x, y in t_bar:\n                x, y = x.to(device), y.to(device)\n                outputs = model(x)\n                loss = criterion(outputs, y)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                total_loss += loss.item()\n                predicted = torch.argmax(outputs, dim=-1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n\n                # Update tqdm progress bar description\n                t_bar.set_description(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n                t_bar.set_postfix(loss=total_loss / (total + 1e-8), accuracy=100 * correct / total)\n        acc = 100.0 * correct / total\n        if acc > max_acc:\n            torch.save(model.state_dict(), f\"./{acc}.pt\")\n        if last_acc - acc > ACC_THRESH:\n            const_epochs = 0\n        else:\n            const_epochs += 1\n        if const_epochs == CONVERGENCE_THRESH:\n            break\n        last_acc = acc\n                \n        # Print epoch-level information\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Loss: {total_loss / (total + 1e-8):.4f}, Accuracy: {100 * correct / total:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:14:31.211603Z","iopub.execute_input":"2023-10-03T20:14:31.212295Z","iopub.status.idle":"2023-10-03T20:16:47.126671Z","shell.execute_reply.started":"2023-10-03T20:14:31.212250Z","shell.execute_reply":"2023-10-03T20:16:47.125650Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"using cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/40: 100%|██████████| 1200/1200 [00:30<00:00, 39.21batch/s, accuracy=80.8, loss=0.0117]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/40 - Loss: 0.0117, Accuracy: 80.75%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/40: 100%|██████████| 1200/1200 [00:25<00:00, 47.15batch/s, accuracy=92, loss=0.00503]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/40 - Loss: 0.0050, Accuracy: 92.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/40: 100%|██████████| 1200/1200 [00:25<00:00, 46.90batch/s, accuracy=94.4, loss=0.00345]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/40 - Loss: 0.0034, Accuracy: 94.44%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/40: 100%|██████████| 1200/1200 [00:25<00:00, 47.16batch/s, accuracy=95.9, loss=0.00252]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/40 - Loss: 0.0025, Accuracy: 95.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/40: 100%|██████████| 1200/1200 [00:25<00:00, 47.27batch/s, accuracy=96.8, loss=0.00195]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:16:57.715127Z","iopub.execute_input":"2023-10-03T20:16:57.715666Z","iopub.status.idle":"2023-10-03T20:16:57.720511Z","shell.execute_reply.started":"2023-10-03T20:16:57.715634Z","shell.execute_reply":"2023-10-03T20:16:57.719634Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"ViT(\n  (encoders): Sequential(\n    (0): Encoder(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (attention): MultiHeadAttention(\n        (Q): Linear(in_features=512, out_features=512, bias=True)\n        (K): Linear(in_features=512, out_features=512, bias=True)\n        (V): Linear(in_features=512, out_features=512, bias=True)\n        (softmax): Softmax(dim=-1)\n        (mlp): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=512, out_features=512, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=512, out_features=512, bias=True)\n      )\n    )\n    (1): Encoder(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (attention): MultiHeadAttention(\n        (Q): Linear(in_features=512, out_features=512, bias=True)\n        (K): Linear(in_features=512, out_features=512, bias=True)\n        (V): Linear(in_features=512, out_features=512, bias=True)\n        (softmax): Softmax(dim=-1)\n        (mlp): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=512, out_features=512, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=512, out_features=512, bias=True)\n      )\n    )\n    (2): Encoder(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (attention): MultiHeadAttention(\n        (Q): Linear(in_features=512, out_features=512, bias=True)\n        (K): Linear(in_features=512, out_features=512, bias=True)\n        (V): Linear(in_features=512, out_features=512, bias=True)\n        (softmax): Softmax(dim=-1)\n        (mlp): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=512, out_features=512, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=512, out_features=512, bias=True)\n      )\n    )\n  )\n  (mlp): Linear(in_features=512, out_features=10, bias=True)\n  (patchify): PatchEmbedding(\n    (projection): Conv2d(1, 512, kernel_size=(7, 7), stride=(7, 7))\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n    with tqdm(test_loader, unit=\"batch\") as t_bar:\n        for x, y in t_bar:\n            x, y = x.to(device), y.to(device)\n            outputs = model(x)\n            predicted = torch.argmax(outputs, dim=-1)\n            total += y.size(0)\n            correct += (predicted == y).sum().item()\n\n            # Update tqdm progress bar description\n            t_bar.set_description(f\"Testing...\")\n            t_bar.set_postfix(loss=total_loss / (total + 1e-8), accuracy=100 * correct / total)\n    acc = 100.0 * correct / total\n    print(f\"Accuracy: {100.0 * correct / total:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:17:09.976861Z","iopub.execute_input":"2023-10-03T20:17:09.977179Z","iopub.status.idle":"2023-10-03T20:17:12.725485Z","shell.execute_reply.started":"2023-10-03T20:17:09.977152Z","shell.execute_reply":"2023-10-03T20:17:12.724595Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Testing...: 100%|██████████| 200/200 [00:02<00:00, 73.04batch/s, accuracy=96.4, loss=0.00167]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 96.39%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}